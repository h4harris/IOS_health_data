{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b70496e8",
      "metadata": {
        "id": "b70496e8"
      },
      "source": [
        "# Strava: Guide to data extraction and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f87e689",
      "metadata": {
        "id": "9f87e689"
      },
      "source": [
        "<img src='https://i.imgur.com/jneRMpU.jpg' height=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ae9032",
      "metadata": {
        "id": "38ae9032"
      },
      "source": [
        "A picture of the Strava Mobile Application\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cf65040",
      "metadata": {
        "id": "9cf65040"
      },
      "source": [
        "Strava is often regarded as the “social network for athletes.” It lets you track your running and riding with GPS, join Challenges, share photos from your activities, and follow friends. <br>\n",
        "\n",
        "Strava follows a fremium model offering a digital service accessible through its mobile applications (iOS and Android). Users also have an option to upgrade and unlock more advanced features like Custom Goals, Training Plans, Race Analysis, etc for a monthly fee of $5-8. <br>\n",
        "\n",
        "We've been using the strava application for the past few weeks and we will show you how to extract its data, visualize your runs and compute correlations between multiple metrics of the data. The Strava API allows the users to extract all sorts of data on athletes, segments, routes, clubs, and gear. However, for this notebook we will be focusing on metrics of the participant's activities like dates, speed, elevation, duration, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8f20a5",
      "metadata": {
        "id": "7d8f20a5"
      },
      "source": [
        "We will be able to extract the following parameters:\n",
        "\n",
        "Parameter Name  | Sampling Frequency\n",
        "-------------------|-----------------\n",
        "<b>Moving Time</b> |  Per Activity\n",
        "<b>Elapsed Time</b> |  Per Activity\n",
        "<b>Average Speed</b> |  Per Activity\n",
        "<b>Maximum Speed</b> |  Per Activity\n",
        "<b>Average Cadence</b> |  Per Activity\n",
        "<b>Maximum Cadence</b> |  Per Activity\n",
        "<b>Average Watts</b> |  Per Activity\n",
        "<b>Maximum Watts</b> |  Per Activity\n",
        "<b>Average Heart Rate</b> |  Per Activity\n",
        "<b>Maximum Heartrate</b> |  Per Activity\n",
        "<b>Distance</b>      |  Records every change in user's position\n",
        "<b>Polyline Summary</b> |  Records every change in user's position\n",
        "<b>Total Elevation Gain</b> |  Sampling Frequency depends upon user's fitness tracker\n",
        "<b>Heart Rate</b>  | Sampling Frequency depends upon user's fitness tracker"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ebad13",
      "metadata": {
        "id": "73ebad13"
      },
      "source": [
        "** All of these are individual paramaters that can be directly extracted using wearipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f03f1a22",
      "metadata": {
        "id": "f03f1a22"
      },
      "source": [
        "In this guide, we sequentially cover the following **five** topics to extract data from Cronometer servers:\n",
        "\n",
        "1. **Set up**<br>\n",
        "2. **Authentication/Authorization**<br>\n",
        "   - Requires only client_id, client_secret and refresh_token.<br>\n",
        "3. **Data extraction**<br>\n",
        "  - We get data via wearipedia in a couple lines of code<br>\n",
        "4. **Data Exporting**\n",
        "    - We export all of this data to file formats compatible by R, Excel, and MatLab.\n",
        "5. **Adherence**\n",
        "    - We simulate non-adherence by dynamically removing datapoints from our simulated data.\n",
        "6. **Visualization**\n",
        "    - We create a simple plot to visualize our data.\n",
        "7. **Advanced visualization**\n",
        "    - 7.1 Visualizing participant's Overall Activity!<br>\n",
        "    - 7.2 Visualizing participant's Weekly Summary!<br>\n",
        "    - 7.3 Visualizing Participant's Runs!<br>\n",
        "8. **Statistical Data Analysis** <br>\n",
        "  - 8.1  Analyzing correlation between Participant's data! <br>\n",
        "9. **Outlier Detection and Data Cleaning** <br>\n",
        "  - 9.1 Highlighting Outliers!\n",
        "\n",
        "Disclaimer: this notebook is purely for educational purposes. All of the data currently stored in this notebook is purely *synthetic*, meaning randomly generated according to rules we created. Despite this, the end-to-end data extraction pipeline has been tested on our own data, meaning that if you enter your own email and password on your own Colab instance, you can visualize your own *real* data. That being said, we were unable to thoroughly test the timezone functionality, though, since we only have one account, so beware."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "675c5b05",
      "metadata": {
        "id": "675c5b05"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "## Participant Setup\n",
        "\n",
        "Dear Participant,\n",
        "\n",
        "Once you download the strava app, please set it up by following these resources:\n",
        "- Written guide: https://www.runnersworld.com/beginner/g25619156/what-is-strava/\n",
        "- Video guide: https://www.youtube.com/watch?v=LHtCxdrZFJ8&ab_channel=RunWithJ\n",
        "\n",
        "Make sure that your phone is logged to the strava app using the Strava login credentials (email and password) given to you by the data receiver.\n",
        "\n",
        "Best,\n",
        "\n",
        "Wearipedia\n",
        "\n",
        "## Data Receiver Setup\n",
        "\n",
        "Please follow the below steps:\n",
        "\n",
        "1. Create an email address for the participant, for example `foo@email.com`.\n",
        "2. Create a Strava account with the email `foo@email.com` and some random password.\n",
        "3. Keep `foo@email.com` and password stored somewhere safe.\n",
        "4. Distribute the device to the participant and instruct them to follow the participant setup letter above.\n",
        "5. Next, go to https://developers.strava.com/\n",
        "6. Click on \"Create and Manage your App\"\n",
        "<img src='https://i.imgur.com/nBdQDR6.png' width=\"850\" height=\"500\">\n",
        "7. Strava will prompt you to login. Make sure to login with the account that participants' credentials\n",
        "8. Fill out your details:<br>\n",
        "   Here's an example with some dummy information! <br>\n",
        "   <img src='https://i.imgur.com/O9jRXQQ.png' width=\"450\" height=\"400\">\n",
        "9. Next, Strava will ask you to upload a app icon. I personally uploaded the Strava logo in the option. <br>\n",
        "    <img src='https://i.imgur.com/wQHGyL2.png' width=\"600\" height=\"400\"> <br>\n",
        "   Copy and crop the Image to your preference and hit save! <br>\n",
        "    <img src='https://i.imgur.com/kKodoZm.png' width=\"600\" height=\"400\">\n",
        "10. <img src='https://i.imgur.com/gV2tGlK.png' width=\"600\" height=\"400\">\n",
        "    \n",
        "    Copy and paste your your client ID and Client Token in the box below and give it to the researcher.\n",
        "    There is an access token and refresh token on the profile page, but it does not have the scope that we want.\n",
        "11. Modify the link below with your client ID and navigate to\n",
        "\n",
        "    \"https://www.strava.com/oauth/authorize?client_id={YOUR_CLIENT_ID}&response_type=code&redirect_uri=http://localhost/exchange_token&approval_prompt=force&scope=read_all,profile:read_all,activity:read_all\"\n",
        "\n",
        "    You should see a page asking you to authorize your app with the following scopes:\n",
        "    <img src='https://i.imgur.com/oEugtqT.png' width=\"600\" height=\"400\">\n",
        "\n",
        "    Click authorize. This will redirect you to a \"This site can't be reached page\". This is expected, but copy the \"code\" parameter out of the URL. For example:\n",
        "\n",
        "    http://localhost/exchange_token?state=&code=13ced876511c89c9c3fb00e8d8898eb92e6102a2&scope=read,activity:read_all,profile:read_all,read_all -> \"13ced876511c89c9c3fb00e8d8898eb92e6102a2\"\n",
        "\n",
        "    Give this to the researcher as well.\n",
        "    \n",
        "13. Install the `wearipedia` Python package to easily extract data from this device via the Strava API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e5f0aab9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5f0aab9",
        "outputId": "680e22f9-288c-4c2a-8834-6fc6553ca828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wearipedia\n",
            "  Downloading wearipedia-0.1.7-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from wearipedia) (4.13.5)\n",
            "Collecting fastapi==0.101 (from wearipedia)\n",
            "  Downloading fastapi-0.101.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting fbm>=0.3.0 (from wearipedia)\n",
            "  Downloading fbm-0.3.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting garminconnect>=0.2.25 (from wearipedia)\n",
            "  Downloading garminconnect-0.2.36-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting garth>=0.5.2 (from wearipedia)\n",
            "  Downloading garth-0.5.19-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting jupyter>=1.0.0 (from wearipedia)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting myfitnesspal>=2.0.1 (from wearipedia)\n",
            "  Downloading myfitnesspal-2.1.2-py3-none-any.whl.metadata (927 bytes)\n",
            "Requirement already satisfied: numpy>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wearipedia) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from wearipedia) (2.2.2)\n",
            "Collecting polyline>=2.0.0 (from wearipedia)\n",
            "  Downloading polyline-2.0.4-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting qualtricsapi>=0.6.1 (from wearipedia)\n",
            "  Downloading QualtricsAPI-0.6.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from wearipedia) (13.9.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wearipedia) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from wearipedia) (4.67.1)\n",
            "Requirement already satisfied: typer>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from typer[all]>=0.6.1->wearipedia) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from wearipedia) (4.15.0)\n",
            "Collecting wget>=3.2 (from wearipedia)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi==0.101->wearipedia) (2.12.3)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi==0.101->wearipedia)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.12.2->wearipedia) (2.8)\n",
            "Requirement already satisfied: requests-oauthlib<3.0.0,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from garth>=0.5.2->wearipedia) (2.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from garth>=0.5.2->wearipedia) (2.32.4)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->wearipedia) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->wearipedia) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->wearipedia) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->wearipedia) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->wearipedia) (7.7.1)\n",
            "Collecting jupyterlab (from jupyter>=1.0.0->wearipedia)\n",
            "  Downloading jupyterlab-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting blessed<2.0,>=1.8.5 (from myfitnesspal>=2.0.1->wearipedia)\n",
            "  Downloading blessed-1.25.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lxml<6,>=5.0.2 (from myfitnesspal>=2.0.1->wearipedia)\n",
            "  Downloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting measurement<4.0,>=3.2.0 (from myfitnesspal>=2.0.1->wearipedia)\n",
            "  Downloading measurement-3.2.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.4 in /usr/local/lib/python3.12/dist-packages (from myfitnesspal>=2.0.1->wearipedia) (2.9.0.post0)\n",
            "Collecting rich>=12.6.0 (from wearipedia)\n",
            "  Downloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting browser_cookie3<1,>=0.16.1 (from myfitnesspal>=2.0.1->wearipedia)\n",
            "  Downloading browser_cookie3-0.20.1-py3-none-any.whl.metadata (713 bytes)\n",
            "Collecting cloudscraper<2,>=1.2.71 (from myfitnesspal>=2.0.1->wearipedia)\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from wearipedia)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->wearipedia) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->wearipedia) (2025.2)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich>=12.6.0->wearipedia)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->wearipedia) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.6.1->typer[all]>=0.6.1->wearipedia) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.6.1->typer[all]>=0.6.1->wearipedia) (1.5.4)\n",
            "\u001b[33mWARNING: typer 0.20.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.12/dist-packages (from blessed<2.0,>=1.8.5->myfitnesspal>=2.0.1->wearipedia) (0.2.14)\n",
            "Collecting lz4 (from browser_cookie3<1,>=0.16.1->myfitnesspal>=2.0.1->wearipedia)\n",
            "  Downloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.12/dist-packages (from browser_cookie3<1,>=0.16.1->myfitnesspal>=2.0.1->wearipedia) (3.23.0)\n",
            "Requirement already satisfied: jeepney in /usr/local/lib/python3.12/dist-packages (from browser_cookie3<1,>=0.16.1->myfitnesspal>=2.0.1->wearipedia) (0.9.0)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.12/dist-packages (from cloudscraper<2,>=1.2.71->myfitnesspal>=2.0.1->wearipedia) (3.2.5)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from cloudscraper<2,>=1.2.71->myfitnesspal>=2.0.1->wearipedia) (1.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from measurement<4.0,>=3.2.0->myfitnesspal>=2.0.1->wearipedia) (1.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.101->wearipedia) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.101->wearipedia) (2.41.4)\n",
            "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi==0.101->wearipedia)\n",
            "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.101->wearipedia)\n",
            "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi==0.101->wearipedia)\n",
            "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading pydantic-2.12.2-py3-none-any.whl.metadata (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading pydantic-2.12.0-py3-none-any.whl.metadata (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.41.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.101->wearipedia)\n",
            "  Downloading pydantic_core-2.41.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi==0.101->wearipedia)\n",
            "  Downloading pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.101->wearipedia)\n",
            "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.101->wearipedia) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.4->myfitnesspal>=2.0.1->wearipedia) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->garth>=0.5.2->wearipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->garth>=0.5.2->wearipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->garth>=0.5.2->wearipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->garth>=0.5.2->wearipedia) (2025.11.12)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib<3.0.0,>=1.3.1->garth>=0.5.2->wearipedia) (3.3.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.101->wearipedia) (4.12.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter>=1.0.0->wearipedia) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->wearipedia) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->wearipedia) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets->jupyter>=1.0.0->wearipedia) (3.0.16)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter>=1.0.0->wearipedia) (5.9.1)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/local/lib/python3.12/dist-packages (from jupyter-console->jupyter>=1.0.0->wearipedia) (3.0.52)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->wearipedia)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->wearipedia) (0.28.1)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->wearipedia) (3.1.6)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->wearipedia)\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->wearipedia) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab->jupyter>=1.0.0->wearipedia)\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->wearipedia) (0.2.4)\n",
            "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->wearipedia) (75.2.0)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->wearipedia) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->wearipedia) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->wearipedia) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->wearipedia) (3.0.3)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->wearipedia) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->wearipedia) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->wearipedia) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->wearipedia) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter>=1.0.0->wearipedia) (25.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter>=1.0.0->wearipedia) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter>=1.0.0->wearipedia) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter>=1.0.0->wearipedia) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->jupyter>=1.0.0->wearipedia) (1.3.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->wearipedia) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->wearipedia) (1.4.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->wearipedia) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->wearipedia) (0.16.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->wearipedia)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->wearipedia) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->wearipedia) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->wearipedia) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->wearipedia) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter>=1.0.0->wearipedia) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter>=1.0.0->wearipedia) (4.5.1)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (1.9.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->wearipedia) (25.1.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->wearipedia) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->wearipedia)\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->wearipedia) (4.25.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->wearipedia) (2.21.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0->wearipedia) (0.7.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->measurement<4.0,>=3.2.0->myfitnesspal>=2.0.1->wearipedia) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->wearipedia) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->wearipedia) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->wearipedia) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->wearipedia) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->wearipedia) (0.30.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (0.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->wearipedia) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->wearipedia) (2.23)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->wearipedia) (1.4.0)\n",
            "Downloading wearipedia-0.1.7-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.9/148.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.101.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fbm-0.3.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading garminconnect-0.2.36-py3-none-any.whl (31 kB)\n",
            "Downloading garth-0.5.19-py3-none-any.whl (33 kB)\n",
            "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading myfitnesspal-2.1.2-py3-none-any.whl (25 kB)\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading polyline-2.0.4-py3-none-any.whl (7.2 kB)\n",
            "Downloading QualtricsAPI-0.6.2-py3-none-any.whl (35 kB)\n",
            "Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blessed-1.25.0-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading browser_cookie3-0.20.1-py3-none-any.whl (17 kB)\n",
            "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading measurement-3.2.2-py3-none-any.whl (17 kB)\n",
            "Downloading pydantic-2.11.10-py3-none-any.whl (444 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab-4.5.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=863a11ee58bfc995822047df22efdca5dd2e3ea63ddaf742d689dfde4e35c735\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, commonmark, typing-extensions, rich, polyline, lz4, lxml, json5, jedi, fbm, blessed, async-lru, pydantic-core, measurement, browser_cookie3, starlette, qualtricsapi, pydantic, cloudscraper, myfitnesspal, garth, fastapi, garminconnect, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, wearipedia\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 6.0.2\n",
            "    Uninstalling lxml-6.0.2:\n",
            "      Successfully uninstalled lxml-6.0.2\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.41.4\n",
            "    Uninstalling pydantic_core-2.41.4:\n",
            "      Successfully uninstalled pydantic_core-2.41.4\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.48.0\n",
            "    Uninstalling starlette-0.48.0:\n",
            "      Successfully uninstalled starlette-0.48.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.3\n",
            "    Uninstalling pydantic-2.12.3:\n",
            "      Successfully uninstalled pydantic-2.12.3\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.118.3\n",
            "    Uninstalling fastapi-0.118.3:\n",
            "      Successfully uninstalled fastapi-0.118.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
            "google-adk 1.20.0 requires fastapi<0.119.0,>=0.115.0, but you have fastapi 0.101.0 which is incompatible.\n",
            "google-adk 1.20.0 requires starlette<1.0.0,>=0.46.2, but you have starlette 0.27.0 which is incompatible.\n",
            "gradio 5.50.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.101.0 which is incompatible.\n",
            "gradio 5.50.0 requires starlette<1.0,>=0.40.0, but you have starlette 0.27.0 which is incompatible.\n",
            "pymc 5.26.1 requires rich>=13.7.1, but you have rich 12.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed async-lru-2.0.5 blessed-1.25.0 browser_cookie3-0.20.1 cloudscraper-1.2.71 commonmark-0.9.1 fastapi-0.101.0 fbm-0.3.0 garminconnect-0.2.36 garth-0.5.19 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.5.0 jupyterlab-server-2.28.0 lxml-5.4.0 lz4-4.4.5 measurement-3.2.2 myfitnesspal-2.1.2 polyline-2.0.4 pydantic-2.11.10 pydantic-core-2.33.2 qualtricsapi-0.6.2 rich-12.6.0 starlette-0.27.0 typing-extensions-4.12.2 wearipedia-0.1.7 wget-3.2\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wearipedia\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2f9f55a1",
      "metadata": {
        "id": "2f9f55a1"
      },
      "outputs": [],
      "source": [
        "import wearipedia\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe816960",
      "metadata": {
        "id": "fe816960"
      },
      "source": [
        "# 2. Authentication and Authorization\n",
        "\n",
        "To obtain access to data, authorization is required. Put in your client id, client secret token and refresh token for your Strava account. We'll use this to extract the data in the sections below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08fbee4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fbee4b",
        "outputId": "82214a15-0b0d-4b98-bfb8-65b8b7d9aa29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error making request: 400 Client Error: Bad Request for url: https://www.strava.com/api/v3/oauth/token\n",
            "Your refresh_token is None\n"
          ]
        }
      ],
      "source": [
        "#@title Enter Strava API credentials\n",
        "client_id = \"189615\" #@param {type:\"string\"}\n",
        "client_secret = \"7eb8a587772e9d1c939857cd7809716c0344e809\" #@param {type:\"string\"}\n",
        "code = \"e06f58212db8ba57d110cd22127c04d81495dd41\" #@param {type:\"string\"}\n",
        "\n",
        "def get_strava_refresh_token(client_id, client_secret, code):\n",
        "    \"\"\"\n",
        "    Get a refresh token from Strava API using authorization code.\n",
        "\n",
        "    Args:\n",
        "        client_id (str): Your Strava API client ID\n",
        "        client_secret (str): Your Strava API client secret\n",
        "        code (str): Authorization code received from Strava\n",
        "\n",
        "    Returns:\n",
        "        str: The refresh token\n",
        "    \"\"\"\n",
        "    url = \"https://www.strava.com/api/v3/oauth/token\"\n",
        "    payload = {\n",
        "        'client_id': client_id,\n",
        "        'client_secret': client_secret,\n",
        "        'code': code,\n",
        "        'grant_type': 'authorization_code'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, data=payload)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        refresh_token = data.get('refresh_token')\n",
        "\n",
        "        if not refresh_token:\n",
        "            print(\"Warning: No refresh token found in response\")\n",
        "        return refresh_token\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making request: {e}\")\n",
        "        return None\n",
        "\n",
        "# If you have a refresh token already directly, you can fill this in instead of calling the above function.\n",
        "refresh_token = get_strava_refresh_token(client_id, client_secret, code)\n",
        "print(\"Your refresh_token is\", refresh_token)\n",
        "\n",
        "# Note that the authorization code can only be used once, so you should hold onto this refresh token, or you can go through the authorize page"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23fec1a3",
      "metadata": {
        "id": "23fec1a3"
      },
      "source": [
        "# 3. Data Extraction\n",
        "\n",
        "Data can be extracted via [wearipedia](https://github.com/Stanford-Health/wearipedia/), our open-source Python package that unifies dozens of complex wearable device APIs into one simple, common interface.\n",
        "\n",
        "First, we'll set a date range and then extract all of the data within that date range. You can select whether you would like synthetic data or not with the checkbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "97175015",
      "metadata": {
        "id": "97175015"
      },
      "outputs": [],
      "source": [
        "#@title Enter start and end dates (in the format yyyy-mm-dd)\n",
        "\n",
        "#set start and end dates - this will give you all the data from 2000-01-01 (January 1st, 2000) to 2100-02-03 (February 3rd, 2100), for example\n",
        "start_date='2022-03-01' #@param {type:\"string\"}\n",
        "end_date='2026-06-17' #@param {type:\"string\"}\n",
        "synthetic = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fdd01256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "fdd01256",
        "outputId": "b49d02ae-747b-48ff-a17f-e8c6336822db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requesting Token...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Strava' object has no attribute 'access_token'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2919529193.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start_date\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_date\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distance\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmoving_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"moving_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"elapsed_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wearipedia/devices/device.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, data_type, params)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthetic_has_been_generated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wearipedia/devices/strava/strava.py\u001b[0m in \u001b[0;36m_get_real\u001b[0;34m(self, data_type, params)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             )\n\u001b[0;32m--> 108\u001b[0;31m         return fetch_real_data(\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wearipedia/devices/strava/strava_fetch.py\u001b[0m in \u001b[0;36mfetch_real_data\u001b[0;34m(self, start_date, end_date, data_type, id)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Header that sends the Access Token in the GET request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Bearer \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     param = {\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m\"per_page\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPER_PAGE_LIMIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Strava' object has no attribute 'access_token'"
          ]
        }
      ],
      "source": [
        "device = wearipedia.get_device(\"strava/strava\")\n",
        "\n",
        "if not synthetic:\n",
        "    device.authenticate({\n",
        "    'client_id':client_id,\n",
        "    'client_secret':client_secret,\n",
        "    'refresh_token':refresh_token\n",
        "    })\n",
        "\n",
        "params = {\"start_date\": start_date, \"end_date\": end_date}\n",
        "\n",
        "distance = device.get_data(\"distance\", params=params)\n",
        "moving_time = device.get_data(\"moving_time\", params=params)\n",
        "elapsed_time = device.get_data(\"elapsed_time\", params=params)\n",
        "total_elevation_gain = device.get_data(\"total_elevation_gain\", params=params)\n",
        "average_speed = device.get_data(\"average_speed\", params=params)\n",
        "max_speed = device.get_data(\"max_speed\", params=params)\n",
        "average_heartrate = device.get_data(\"average_heartrate\", params=params)\n",
        "max_heartrate = device.get_data(\"max_heartrate\", params=params)\n",
        "map_summary_polyline = device.get_data(\"map_summary_polyline\", params=params)\n",
        "elev_high = device.get_data(\"elev_high\", params=params)\n",
        "elev_low = device.get_data(\"elev_low\", params=params)\n",
        "average_cadence = device.get_data(\"average_cadence\", params=params)\n",
        "average_watts = device.get_data(\"average_watts\", params=params)\n",
        "kilojoules = device.get_data(\"kilojoules\", params=params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44bbc267",
      "metadata": {
        "id": "44bbc267"
      },
      "source": [
        "# 4. Data Exporting\n",
        "\n",
        "In this section, we export all of this data to formats compatible with popular scientific computing software (R, Excel, Google Sheets, Matlab). Specifically, we will first export to JSON, which can be read by R and Matlab. Then, we will export to CSV, which can be consumed by Excel, Google Sheets, and every other popular programming language.\n",
        "\n",
        "## Exporting to JSON (R, Matlab, etc.)\n",
        "\n",
        "Exporting to JSON is fairly simple. We export each datatype separately and also export a complete version that includes all simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ab6600",
      "metadata": {
        "id": "41ab6600"
      },
      "outputs": [],
      "source": [
        "def datacleanup(data):\n",
        "    for d in data:\n",
        "        d['start_date'] = str(d['start_date'])\n",
        "    return data\n",
        "\n",
        "json.dump(datacleanup(distance), open(\"distance.json\", \"w\"))\n",
        "json.dump(datacleanup(moving_time), open(\"moving_time.json\", \"w\"))\n",
        "json.dump(datacleanup(elapsed_time), open(\"elapsed_time.json\", \"w\"))\n",
        "json.dump(datacleanup(total_elevation_gain), open(\"total_elevation_gain.json\", \"w\"))\n",
        "json.dump(datacleanup(average_speed), open(\"average_speed.json\", \"w\"))\n",
        "json.dump(datacleanup(max_speed), open(\"max_speed.json\", \"w\"))\n",
        "json.dump(datacleanup(average_heartrate), open(\"average_heartrate.json\", \"w\"))\n",
        "json.dump(datacleanup(max_heartrate), open(\"max_heartrate.json\", \"w\"))\n",
        "json.dump(datacleanup(map_summary_polyline), open(\"map_summary_polyline.json\", \"w\"))\n",
        "json.dump(datacleanup(elev_high), open(\"elev_high.json\", \"w\"))\n",
        "json.dump(datacleanup(elev_low), open(\"elev_low.json\", \"w\"))\n",
        "json.dump(datacleanup(average_cadence), open(\"average_cadence.json\", \"w\"))\n",
        "json.dump(datacleanup(average_watts), open(\"average_watts.json\", \"w\"))\n",
        "json.dump(datacleanup(kilojoules), open(\"kilojoules.json\", \"w\"))\n",
        "\n",
        "\n",
        "complete = {\n",
        "    \"distance\": distance,\n",
        "    'moving_time':moving_time,\n",
        "    'elapsed_time':elapsed_time,\n",
        "    'total_elevation_gain':total_elevation_gain,\n",
        "    'average_speed':average_speed,\n",
        "    'max_speed':max_speed,\n",
        "    'average_heartrate':average_heartrate,\n",
        "    'max_heartrate':max_heartrate,\n",
        "    'map_summary_polyline':map_summary_polyline,\n",
        "    'elev_high':elev_high,\n",
        "    'elev_low':elev_low,\n",
        "    'average_cadence':average_cadence,\n",
        "    'average_watts':average_watts,\n",
        "    'kilojoules':kilojoules\n",
        "}\n",
        "\n",
        "json.dump(complete, open(\"complete.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a25f19",
      "metadata": {
        "id": "59a25f19"
      },
      "source": [
        "Feel free to open the file viewer (see left pane) to look at the outputs!\n",
        "\n",
        "## Exporting to CSV and XLSX (Excel, Google Sheets, R, Matlab, etc.)\n",
        "\n",
        "Exporting to CSV/XLSX requires a bit more processing, since they enforce a pretty restrictive schema.\n",
        "\n",
        "We will thus export steps, heart rates, and breath rates all as separate files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "169bb66b",
      "metadata": {
        "id": "169bb66b"
      },
      "outputs": [],
      "source": [
        "distance_df = pd.DataFrame.from_dict(complete['distance'])\n",
        "distance_df.to_csv('distance.csv')\n",
        "distance_df.to_excel('distance.xlsx')\n",
        "\n",
        "moving_time_df = pd.DataFrame.from_dict(complete['moving_time'])\n",
        "moving_time_df.to_csv('moving_time.csv')\n",
        "moving_time_df.to_excel('moving_time.xlsx')\n",
        "\n",
        "elapsed_time_df = pd.DataFrame.from_dict(complete['elapsed_time'])\n",
        "elapsed_time_df.to_csv('elapsed_time.csv')\n",
        "elapsed_time_df.to_excel('elapsed_time.xlsx')\n",
        "\n",
        "total_elevation_gain_df = pd.DataFrame.from_dict(complete['total_elevation_gain'])\n",
        "total_elevation_gain_df.to_csv('total_elevation_gain.csv')\n",
        "total_elevation_gain_df.to_excel('total_elevation_gain.xlsx')\n",
        "\n",
        "average_speed_df = pd.DataFrame.from_dict(complete['average_speed'])\n",
        "average_speed_df.to_csv('average_speed.csv')\n",
        "average_speed_df.to_excel('average_speed.xlsx')\n",
        "\n",
        "max_speed_df = pd.DataFrame.from_dict(complete['max_speed'])\n",
        "max_speed_df.to_csv('max_speed.csv')\n",
        "max_speed_df.to_excel('max_speed.xlsx')\n",
        "\n",
        "average_heartrate_df = pd.DataFrame.from_dict(complete['average_heartrate'])\n",
        "average_heartrate_df.to_csv('average_heartrate.csv')\n",
        "average_heartrate_df.to_excel('average_heartrate.xlsx')\n",
        "\n",
        "max_heartrate_df = pd.DataFrame.from_dict(complete['max_heartrate'])\n",
        "max_heartrate_df.to_csv('max_heartrate.csv')\n",
        "max_heartrate_df.to_excel('max_heartrate.xlsx')\n",
        "\n",
        "map_summary_polyline_df = pd.DataFrame.from_dict(complete['map_summary_polyline'])\n",
        "map_summary_polyline_df.to_csv('map_summary_polyline.csv')\n",
        "map_summary_polyline_df.to_excel('map_summary_polyline.xlsx')\n",
        "\n",
        "elev_high_df = pd.DataFrame.from_dict(complete['elev_high'])\n",
        "elev_high_df.to_csv('elev_high.csv')\n",
        "elev_high_df.to_excel('elev_high.xlsx')\n",
        "\n",
        "elev_low_df = pd.DataFrame.from_dict(complete['elev_low'])\n",
        "elev_low_df.to_csv('elev_low.csv')\n",
        "elev_low_df.to_excel('elev_low.xlsx')\n",
        "\n",
        "average_cadence_df = pd.DataFrame.from_dict(complete['average_cadence'])\n",
        "average_cadence_df.to_csv('average_cadence.csv')\n",
        "average_cadence_df.to_excel('average_cadence.xlsx')\n",
        "\n",
        "average_watts_df = pd.DataFrame.from_dict(complete['average_watts'])\n",
        "average_watts_df.to_csv('average_watts.csv')\n",
        "average_watts_df.to_excel('average_watts.xlsx')\n",
        "\n",
        "kilojoules_df = pd.DataFrame.from_dict(complete['kilojoules'])\n",
        "kilojoules_df.to_csv('kilojoules.csv')\n",
        "kilojoules_df.to_excel('kilojoules.xlsx')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4b30043",
      "metadata": {
        "id": "b4b30043"
      },
      "source": [
        "Again, feel free to look at the output files and download them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a7b240",
      "metadata": {
        "id": "f8a7b240"
      },
      "source": [
        "# 5. Adherence\n",
        "\n",
        "The device simulator already automatically randomly deletes small chunks of the day. In this section, we will simulate non-adherence over longer periods of time from the participant (day-level and week-level).\n",
        "\n",
        "Then, we will detect this non-adherence and give a Pandas DataFrame that concisely describes when the participant has had their device on and off throughout the entirety of the time period, allowing you to calculate how long they've had it on/off etc.\n",
        "\n",
        "We will first delete a certain % of blocks either at the day level or week level, with user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "350a67b3",
      "metadata": {
        "id": "350a67b3"
      },
      "outputs": [],
      "source": [
        "#@title Non-adherence simulation\n",
        "block_level = \"day\" #@param [\"day\", \"week\"]\n",
        "adherence_percent = 0.89 #@param {type:\"slider\", min:0, max:1, step:0.01}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857c9908",
      "metadata": {
        "id": "857c9908"
      },
      "outputs": [],
      "source": [
        "complete = {\n",
        "    \"distance\": distance,\n",
        "    'moving_time':moving_time,\n",
        "    'elapsed_time':elapsed_time,\n",
        "    'total_elevation_gain':total_elevation_gain,\n",
        "    'average_speed':average_speed,\n",
        "    'max_speed':max_speed,\n",
        "    'average_heartrate':average_heartrate,\n",
        "    'max_heartrate':max_heartrate,\n",
        "    'map_summary_polyline':map_summary_polyline,\n",
        "    'elev_high':elev_high,\n",
        "    'elev_low':elev_low,\n",
        "    'average_cadence':average_cadence,\n",
        "    'average_watts':average_watts,\n",
        "    'kilojoules':kilojoules\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c189e75",
      "metadata": {
        "id": "9c189e75"
      },
      "outputs": [],
      "source": [
        "if block_level == \"day\":\n",
        "    block_length = 1\n",
        "elif block_level == \"week\":\n",
        "    block_length = 7\n",
        "\n",
        "\n",
        "\n",
        "# This function will randomly remove datapoints from the\n",
        "# data we have recieved from Cronometer based on the\n",
        "# adherence_percent\n",
        "\n",
        "def AdherenceSimulator(data):\n",
        "\n",
        "  num_blocks = len(data) // block_length\n",
        "  num_blocks_to_keep = int(adherence_percent * num_blocks)\n",
        "  idxes = np.random.choice(np.arange(num_blocks), replace=False,\n",
        "  size=num_blocks_to_keep)\n",
        "\n",
        "  adhered_data = []\n",
        "\n",
        "  for i in range(len(data)):\n",
        "      if i in idxes:\n",
        "          start = i * block_length\n",
        "          end = (i + 1) * block_length\n",
        "          for j in range(i,i+1):\n",
        "            adhered_data.append(data[j])\n",
        "\n",
        "  return adhered_data\n",
        "\n",
        "\n",
        "# Adding adherence for distance\n",
        "\n",
        "distance = AdherenceSimulator(distance)\n",
        "\n",
        "# Adding adherence for moving_time\n",
        "\n",
        "moving_time = AdherenceSimulator(moving_time)\n",
        "\n",
        "# Adding adherence for elapsed_time\n",
        "\n",
        "elapsed_time = AdherenceSimulator(elapsed_time)\n",
        "\n",
        "# Adding adherence for total_elevation_gain\n",
        "\n",
        "total_elevation_gain = AdherenceSimulator(total_elevation_gain)\n",
        "\n",
        "# Adding adherence for average_speed\n",
        "\n",
        "average_speed = AdherenceSimulator(average_speed)\n",
        "\n",
        "# Adding adherence for max_speed\n",
        "\n",
        "max_speed = AdherenceSimulator(max_speed)\n",
        "\n",
        "# Adding adherence for average_heartrate\n",
        "\n",
        "average_heartrate = AdherenceSimulator(average_heartrate)\n",
        "\n",
        "# Adding adherence for max_heartrate\n",
        "\n",
        "max_heartrate = AdherenceSimulator(max_heartrate)\n",
        "\n",
        "# Adding adherence for map_summary_polyline\n",
        "\n",
        "map_summary_polyline = AdherenceSimulator(map_summary_polyline)\n",
        "\n",
        "# Adding adherence for elev_high\n",
        "\n",
        "elev_high = AdherenceSimulator(elev_high)\n",
        "\n",
        "# Adding adherence for elev_low\n",
        "\n",
        "elev_low = AdherenceSimulator(elev_low)\n",
        "\n",
        "# Adding adherence for average_cadence\n",
        "\n",
        "average_cadence = AdherenceSimulator(average_cadence)\n",
        "\n",
        "# Adding adherence for average_watts\n",
        "\n",
        "average_watts = AdherenceSimulator(average_watts)\n",
        "\n",
        "# Adding adherence for kilojoules\n",
        "\n",
        "kilojoules = AdherenceSimulator(kilojoules)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c617a06c",
      "metadata": {
        "id": "c617a06c"
      },
      "source": [
        "And now we have significantly fewer datapoints! This will give us a more realistic situation, where participants may take off their device for days or weeks at a time.\n",
        "\n",
        "Now let's detect non-adherence. We will return a Pandas DataFrame sampled at every day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f1ddb5",
      "metadata": {
        "id": "35f1ddb5"
      },
      "outputs": [],
      "source": [
        "distance_df = pd.DataFrame.from_dict(distance)\n",
        "moving_time_df = pd.DataFrame.from_dict(moving_time)\n",
        "elapsed_time_df = pd.DataFrame.from_dict(elapsed_time)\n",
        "total_elevation_gain_df = pd.DataFrame.from_dict(total_elevation_gain)\n",
        "average_speed_df = pd.DataFrame.from_dict(average_speed)\n",
        "max_speed_df = pd.DataFrame.from_dict(max_speed)\n",
        "average_heartrate_df = pd.DataFrame.from_dict(average_heartrate)\n",
        "max_heartrate_df = pd.DataFrame.from_dict(max_heartrate)\n",
        "map_summary_polyline_df = pd.DataFrame.from_dict(map_summary_polyline)\n",
        "elev_high_df = pd.DataFrame.from_dict(elev_high)\n",
        "elev_low_df = pd.DataFrame.from_dict(elev_low)\n",
        "average_cadence_df = pd.DataFrame.from_dict(average_cadence)\n",
        "average_watts_df = pd.DataFrame.from_dict(average_watts)\n",
        "kilojoules_df = pd.DataFrame.from_dict(kilojoules)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ee7ad7",
      "metadata": {
        "id": "c6ee7ad7"
      },
      "source": [
        "We can plot this out, and we get adherence at one-day frequency throughout the entirety of the data collection period. For this chart we will plot Distance Traveled per day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4153da9",
      "metadata": {
        "id": "b4153da9"
      },
      "outputs": [],
      "source": [
        "distance_df_daily = distance_df.assign(start_date = distance_df.get('start_date').apply(lambda x: x[:10]))\n",
        "distance_df_daily = distance_df_daily.groupby('start_date').sum(numeric_only=True)\n",
        "\n",
        "dates = pd.date_range(start_date,end_date)\n",
        "\n",
        "energy = []\n",
        "\n",
        "for d in dates:\n",
        "    res = distance_df_daily[distance_df_daily.index == datetime.datetime.strftime(d,\n",
        "    '%Y-%m-%d')]['distance']\n",
        "    if len(res) == 0:\n",
        "        energy.append(None)\n",
        "    else:\n",
        "        energy.append(res.iloc[0])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.lineplot(x=dates, y=energy)\n",
        "plt.ylabel('Distance traveled (m)')\n",
        "plt.xlabel('Date')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f206e8",
      "metadata": {
        "id": "22f206e8"
      },
      "source": [
        "# 6. Visualization\n",
        "\n",
        "We've extracted lots of data, but what does it look like?\n",
        "\n",
        "In this section, we will be visualizing our three kinds of data in a simple, customizable plot! This plot is intended to provide a starter example for plotting, whereas later examples emphasize deep control and aesthetics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5870d43",
      "metadata": {
        "id": "c5870d43"
      },
      "outputs": [],
      "source": [
        "#@title Basic Plot\n",
        "feature = \"total_elevation_gain\" #@param ['moving_time', 'elapsed_time','total_elevation_gain','average_speed']\n",
        "start_date = \"2022-03-04\" #@param {type:\"date\"}\n",
        "time_interval = \"full time\" #@param [\"one week\", \"full time\"]\n",
        "smoothness = 0.02 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "smooth_plot = True #@param {type:\"boolean\"}\n",
        "\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "\n",
        "if time_interval == \"one week\":\n",
        "    day_idxes = [i for i,d in enumerate(dates) if d >= start_date and d <= start_date + timedelta(days=7)]\n",
        "    end_date = start_date + timedelta(days=7)\n",
        "elif time_interval == \"full time\":\n",
        "    day_idxes = [i for i,d in enumerate(dates) if d >= start_date]\n",
        "    end_date = dates[-1]\n",
        "\n",
        "if feature == \"moving_time\":\n",
        "    moving_time_daily = moving_time_df.assign(start_date = moving_time_df.get('start_date').apply(lambda x: x[:10]))\n",
        "    moving_time_daily = moving_time_daily.groupby('start_date').sum(numeric_only=True)\n",
        "    concat_moving_time = []\n",
        "    for i,d in enumerate(dates):\n",
        "        day = d.strftime('%Y-%m-%d')\n",
        "        if i in day_idxes:\n",
        "            mt = moving_time_daily[moving_time_daily.index==day]\n",
        "            if len(mt) != 0:\n",
        "                concat_moving_time += [(day,mt.iloc[0].moving_time)]\n",
        "            else:\n",
        "                concat_moving_time += [(day,None)]\n",
        "    ts = [x[0] for x in concat_moving_time]\n",
        "\n",
        "    day_arr = [x[1] for x in concat_moving_time]\n",
        "\n",
        "    sigma = 200 * smoothness\n",
        "\n",
        "    title_fillin = \"Moving Time\"\n",
        "\n",
        "if feature == \"elapsed_time\":\n",
        "    elapsed_time_daily = elapsed_time_df.assign(start_date = elapsed_time_df.get('start_date').apply(lambda x: x[:10]))\n",
        "    elapsed_time_daily = elapsed_time_daily.groupby('start_date').sum(numeric_only=True)\n",
        "    concat_elapsed_time = []\n",
        "    for i,d in enumerate(dates):\n",
        "        day = d.strftime('%Y-%m-%d')\n",
        "        if i in day_idxes:\n",
        "            et = elapsed_time_daily[elapsed_time_daily.index==day]\n",
        "            if len(et) != 0:\n",
        "                concat_elapsed_time += [(day,et.iloc[0].elapsed_time)]\n",
        "            else:\n",
        "                concat_elapsed_time += [(day,None)]\n",
        "    ts = [x[0] for x in concat_elapsed_time]\n",
        "\n",
        "    day_arr = [x[1] for x in concat_elapsed_time]\n",
        "\n",
        "    sigma = 200 * smoothness\n",
        "\n",
        "    title_fillin = \"Elapsed Time\"\n",
        "\n",
        "if feature == \"total_elevation_gain\":\n",
        "    total_elevation_gain_daily = total_elevation_gain_df.assign(start_date = total_elevation_gain_df.get('start_date').apply(lambda x: x[:10]))\n",
        "    total_elevation_gain_daily = total_elevation_gain_daily.groupby('start_date').sum(numeric_only=True)\n",
        "    concat_total_elevation_gain = []\n",
        "    for i,d in enumerate(dates):\n",
        "        day = d.strftime('%Y-%m-%d')\n",
        "        if i in day_idxes:\n",
        "            et = total_elevation_gain_daily[total_elevation_gain_daily.index==day]\n",
        "            if len(et) != 0:\n",
        "                concat_total_elevation_gain += [(day,et.iloc[0].total_elevation_gain)]\n",
        "            else:\n",
        "                concat_total_elevation_gain += [(day,None)]\n",
        "    ts = [x[0] for x in concat_total_elevation_gain]\n",
        "\n",
        "    day_arr = [x[1] for x in concat_total_elevation_gain]\n",
        "\n",
        "    sigma = 200 * smoothness\n",
        "\n",
        "    title_fillin = \"Total Elevation Gain\"\n",
        "\n",
        "if feature == \"average_speed\":\n",
        "    average_speed_daily = average_speed_df.assign(start_date = average_speed_df.get('start_date').apply(lambda x: x[:10]))\n",
        "    average_speed_daily = average_speed_daily.groupby('start_date').sum(numeric_only=True)\n",
        "    concat_average_speed = []\n",
        "    for i,d in enumerate(dates):\n",
        "        day = d.strftime('%Y-%m-%d')\n",
        "        if i in day_idxes:\n",
        "            et = average_speed_daily[average_speed_daily.index==day]\n",
        "            if len(et) != 0:\n",
        "                concat_average_speed += [(day,et.iloc[0].average_speed)]\n",
        "            else:\n",
        "                concat_average_speed += [(day,None)]\n",
        "    ts = [x[0] for x in concat_average_speed]\n",
        "\n",
        "    day_arr = [x[1] for x in concat_average_speed]\n",
        "\n",
        "    sigma = 200 * smoothness\n",
        "\n",
        "    title_fillin = \"Average Speed\"\n",
        "\n",
        "with plt.style.context('ggplot'):\n",
        "    fig, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "    if smooth_plot:\n",
        "        def to_numpy(day_arr):\n",
        "            arr_nonone = [x for x in day_arr if x is not None]\n",
        "            mean_val = int(np.mean(arr_nonone))\n",
        "            for i,x in enumerate(day_arr):\n",
        "                if x is None:\n",
        "                    day_arr[i] = mean_val\n",
        "\n",
        "            return np.array(day_arr)\n",
        "\n",
        "        none_idxes = [i for i,x in enumerate(day_arr) if x is None]\n",
        "        day_arr = to_numpy(day_arr)\n",
        "        from scipy.ndimage import gaussian_filter\n",
        "        day_arr = list(gaussian_filter(day_arr, sigma=sigma))\n",
        "        for i, x in enumerate(day_arr):\n",
        "            if i in none_idxes:\n",
        "                day_arr[i] = None\n",
        "\n",
        "    plt.plot(ts, day_arr)\n",
        "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
        "    plt.title(f\"{title_fillin} from {start_date_str} to {end_date_str}\",\n",
        "              fontsize=20)\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.xticks(ts[::int(len(ts)/8)])\n",
        "    plt.ylabel(title_fillin)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eb3667d",
      "metadata": {
        "id": "6eb3667d"
      },
      "source": [
        "This plot allows you to quickly scan your data at many different time scales (week and full) and for different kinds of measurements (heart rate and weight), which enables easy and fast data exploration.\n",
        "\n",
        "Furthermore, the smoothness parameter makes it easy to look for patterns in long-term trends."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3da7f9",
      "metadata": {
        "id": "1b3da7f9"
      },
      "source": [
        "# 7. Advanced Visualization\n",
        "\n",
        "Now we'll do some more advanced plotting that at times features hardcore matplotlib hacking with the benefit of aesthetic quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b4f134",
      "metadata": {
        "id": "45b4f134"
      },
      "source": [
        "## 7.1 Visualizing participant's Overall Activity!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645c3133",
      "metadata": {
        "id": "645c3133"
      },
      "source": [
        "Whenever our participant is curious and logs into Strava to check their overall summary, the Strava app would present their data in the form of a barchart. It should look something similar to this: <br>\n",
        "<img src=\"https://i.imgur.com/cr5VYpu.png\" height=\"300\">\n",
        " <br>\n",
        "<i>Above is a plot from the app! </i><br> <br>\n",
        "\n",
        "Now that we have user's data, let's try to recreate the chart above using our python skills!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925077c4",
      "metadata": {
        "id": "925077c4"
      },
      "outputs": [],
      "source": [
        "elevation_df = total_elevation_gain_df.assign(total_elevation_gain\n",
        "  = total_elevation_gain_df['total_elevation_gain'].apply(lambda elevation: elevation*3.281))\n",
        "elevation_df = elevation_df.merge(distance_df.get(['distance','start_date']),on='start_date')\n",
        "elevation_df = elevation_df.merge(moving_time_df.get(['moving_time','start_date']),on='start_date')\n",
        "elevation_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6dcef62",
      "metadata": {
        "id": "a6dcef62"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta, date\n",
        "import seaborn as sns\n",
        "\n",
        "#@title Set date range for the chart above\n",
        "\n",
        "start = \"2022-04-25\" #@param {type:\"date\"}\n",
        "end = \"2022-05-25\" #@param {type:\"date\"}\n",
        "\n",
        "#Converting all the elevation gains from metres to feet\n",
        "elevation_df = total_elevation_gain_df.assign(total_elevation_gain\n",
        "  = total_elevation_gain_df['total_elevation_gain'].apply(lambda elevation: elevation*3.281))\n",
        "elevation_df = elevation_df.merge(distance_df.get(['distance','start_date']),on='start_date')\n",
        "elevation_df = elevation_df.merge(moving_time_df.get(['moving_time','start_date']),on='start_date')\n",
        "\n",
        "#Function that converts data into a date time object\n",
        "datefixer = lambda date: datetime.fromisoformat(date[0:10])\n",
        "\n",
        "#Applying datefixer function to every column of elevation_df\n",
        "elevation_df = elevation_df.assign(date\n",
        "                          = elevation_df.get('start_date').apply(datefixer))\n",
        "\n",
        "#Dictionary to store dates froms start to end date along with elevations\n",
        "date_etime = {}\n",
        "\n",
        "#Starting date of our chart\n",
        "start_date = date(int(start.split('-')[0]),int(start.split('-')[1]),\n",
        "                  int(start.split('-')[2]))\n",
        "\n",
        "#Ending date of our chart\n",
        "end_date = date(int(end.split('-')[0]),int(end.split('-')[1]),\n",
        "                int(end.split('-')[2]))\n",
        "\n",
        "#A list of all dates between start and end date\n",
        "dates = list(pd.date_range(start_date,end_date-timedelta(days=1),freq='d'))\n",
        "\n",
        "# Storing toal distance, time and elevation\n",
        "total_distance = 0\n",
        "total_time = 0\n",
        "total_elevation = 0\n",
        "\n",
        "#Loop to find the elevation for each date within the Data Frame\n",
        "for date_val in dates:\n",
        "  #Initializes the current date in the dictionary as zero\n",
        "  date_etime[str(date_val.day)+\" \"+str(date_val.month_name())[:3]] = 0\n",
        "  for actvity_index in range(len(elevation_df)):\n",
        "    #Checks if the current date is in the activity DataFrame\n",
        "      if(date_val == elevation_df.iloc[actvity_index].get('date')):\n",
        "        #Storing total time, distance and elevation\n",
        "        total_distance = (total_distance\n",
        "        + elevation_df.iloc[actvity_index].get('distance'))\n",
        "        total_time = (total_time +\n",
        "               elevation_df.iloc[actvity_index].get('moving_time'))\n",
        "        total_elevation = (total_elevation +\n",
        "               elevation_df.iloc[actvity_index].get('total_elevation_gain'))\n",
        "        #Stores the elevation of current date in dictionary\n",
        "        date_etime[(str(date_val.day)+\" \"+\n",
        "                  str(date_val.month_name())[:3])] = (date_etime[\n",
        "                  str(date_val.day)+\" \"+str(date_val.month_name())[:3]] +\n",
        "                  elevation_df.iloc[actvity_index].get('total_elevation_gain'))\n",
        "#Resetting seaborn to prevent interference with matplotlib plots\n",
        "sns.reset_orig()\n",
        "\n",
        "\n",
        "# custom font\n",
        "# https://stackoverflow.com/questions/35668219/how-to-set-up-a-custom-font-with-custom-path-to-matplotlib-global-font\n",
        "# download the font and unzip (quiet so it does not print)\n",
        "!wget -q 'https://dl.dafont.com/dl/?f=mustica_pro'\n",
        "!unzip -qo \"index.html?f=mustica_pro\"\n",
        "\n",
        "try:\n",
        "    # move to directory where fonts should be kept\n",
        "    !mv -f MusticaPro-SemiBold.otf\n",
        "\n",
        "    # build cache, redirect to /dev/null to suppress stdout output\n",
        "    !fc-cache -f -v > /dev/null\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# try and except, just in case something fails we fallback onto the\n",
        "# default font\n",
        "try:\n",
        "    fe = fm.FontEntry(\n",
        "        #font name\n",
        "        fname='MusticaPro-SemiBold',\n",
        "        name='DejaVu Sans')\n",
        "    fm.fontManager.ttflist.insert(0, fe) # or append is fine\n",
        "    mpl.rcParams['font.family'] = fe.name # = 'your custom ttf font name'\n",
        "except:\n",
        "    pass\n",
        "\n",
        "#Creating a matplotlib plot of size 16,4\n",
        "plt1 = plt.figure(figsize=(16,4))\n",
        "ax = plt1.gca()\n",
        "\n",
        "#Plotting a bar chart with our data in the dictionary\n",
        "plt.bar(date_etime.keys(),date_etime.values(),color=\"#0098DB\", width=0.9)\n",
        "\n",
        "#Adding the title to the chart\n",
        "plt.title( \"$\\\\bf{\"+str(round(total_distance / 1609,1))+\"}$mi  |  \"+\n",
        "          \"$\\\\bf{\"+str(round(total_time/3600))+\"}$h \"+\"$\\\\bf{\"+\n",
        "          str(round(total_time%60))+\"}$m  |  \"\n",
        "          +\"$\\\\bf{\"+str(round(total_elevation))+\"}$ft\")\n",
        "\n",
        "#Only showing xticks for one day/week\n",
        "ax.set_xticks(ax.get_xticks()[::7])\n",
        "\n",
        "#Limiting the y-axis based on our reference chart\n",
        "plt.ylim((0,350))\n",
        "plt.ylabel(\"Total Elevation Gain (Feet)\",color=\"#a1a1a1\")\n",
        "\n",
        "# Removing the spines on top, left and right\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "ax.tick_params(left=False, bottom=True)\n",
        "\n",
        "#Setting the bottom spine to be the same color as the reference chart\n",
        "ax.spines['bottom'].set_color('#0098DB')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c96606",
      "metadata": {
        "id": "04c96606"
      },
      "source": [
        "## 7.2 Visualizing participant's Weekly Summary!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617a9da7",
      "metadata": {
        "id": "617a9da7"
      },
      "source": [
        "If our participant is curious about a more detailed breakdown of their runs, Strava would show you their weekly summary using the following chart: <br>\n",
        "<img src=\"https://trailingclosure.com/content/images/2020/12/IMG_6409.jpg\" height=\"300\">\n",
        " <br>\n",
        "Again, Let's try to recreate it using the data we have fetched from the Strava API<br><br>\n",
        "<b>Note:</b> Weekly summary includes all the runs, walks and rides walks recorded by the participant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef03022",
      "metadata": {
        "id": "7ef03022"
      },
      "source": [
        "Enter the dates for the start and end dates for this chart below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34eb7b18",
      "metadata": {
        "id": "34eb7b18"
      },
      "outputs": [],
      "source": [
        "#@title Set date range for the chart above\n",
        "\n",
        "start = \"2022-04-27\" #@param {type:\"date\"}\n",
        "end = \"2022-05-04\" #@param {type:\"date\"}\n",
        "\n",
        "#Function that converts data into a date time object\n",
        "datefixer = lambda date: datetime.fromisoformat(date[0:10])\n",
        "\n",
        "#Applying datefixer function to every column of df_strava_summary\n",
        "df_strava_summary = elevation_df.assign(date\n",
        "                                =elevation_df.get('start_date').apply(datefixer))\n",
        "\n",
        "#Dictionary to store the hourly frequency\n",
        "date_etime = {}\n",
        "\n",
        "#Starting date of our chart\n",
        "start_date = date(int(start.split('-')[0]),\n",
        "                  int(start.split('-')[1]),int(start.split('-')[2]))\n",
        "\n",
        "#Ending date of our chart\n",
        "end_date = date(int(end.split('-')[0]),\n",
        "                int(end.split('-')[1]),int(end.split('-')[2]))\n",
        "\n",
        "#Creating a list of dates between start and end date\n",
        "dates = list(pd.date_range(start_date,end_date-timedelta(days=1),freq='d'))\n",
        "\n",
        "# Storing toal distance, time and elevation\n",
        "total_distance = 0\n",
        "total_time = 0\n",
        "total_elevation = 0\n",
        "\n",
        "for date_val in dates:\n",
        "  date_etime[str(date_val.day)+\" \"+str(date_val.month_name())[:3]] = 0\n",
        "  for actvity_index in range(len(df_strava_summary)):\n",
        "    #Checks if the current date is in the activity DataFrame\n",
        "      if(date_val == df_strava_summary.iloc[actvity_index].get('date')):\n",
        "\n",
        "        #Storing total time, distance and elevation\n",
        "\n",
        "        total_distance = (total_distance +\n",
        "                          df_strava_summary.iloc[actvity_index].get('distance'))\n",
        "\n",
        "        total_time = (total_time +\n",
        "                      df_strava_summary.iloc[actvity_index].get('moving_time'))\n",
        "\n",
        "        total_elevation = (total_elevation +\n",
        "              df_strava_summary.iloc[actvity_index].get('total_elevation_gain'))\n",
        "\n",
        "        #Stores the elevation of moving time in dictionary\n",
        "        date_etime[str(date_val.day)+\" \"\n",
        "        +str(date_val.month_name())[:3]] = (date_etime[str(date_val.day)\n",
        "        +\" \"+str(date_val.month_name())[:3]] +\n",
        "         df_strava_summary.iloc[actvity_index].get('moving_time')/60)\n",
        "\n",
        "#Resetting seaborn to prevent interference with matplotlib plots\n",
        "sns.reset_orig()\n",
        "\n",
        "# Creating a matplotlib plot of size 16,8\n",
        "plt1 = plt.figure(figsize=(16,8))\n",
        "ax = plt1.gca()\n",
        "\n",
        "\n",
        "plt.plot(list(date_etime.keys()),list(date_etime.values()),color=\"#FC5200\",\n",
        "         marker='o', fillstyle='none', lw=3, markerfacecolor='white', ms=20,\n",
        "         mew=3, markeredgecolor='#FC5200')\n",
        "\n",
        "plt.fill_between(list(date_etime.keys()),\n",
        "                 list(date_etime.values()), facecolor='#FC5200',alpha=0.1)\n",
        "\n",
        "\n",
        "\n",
        "# Adding veritcal grids\n",
        "plt.grid(axis=\"x\")\n",
        "\n",
        "# Hiding the y-ticks\n",
        "ax.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "#Adding the title to the chart\n",
        "plt.suptitle(list(date_etime.keys())[0]+\" - \"+\n",
        "             list(date_etime.keys())[-1],fontsize=24,x=0.195)\n",
        "#Adding the subtitle to the chart\n",
        "plt.title( \"$\\\\bf{\"+str(round(total_distance / 1609,1))+\"}$mi  |  \"+\n",
        "          \"$\\\\bf{\"+str(round(total_time/3600))+\"}$h \"+\"$\\\\bf{\"+\n",
        "          str(round(total_time%60))+\"}$m  |  \"+\"$\\\\bf{\"+\n",
        "          str(round(total_elevation))+\"}$ft\",fontsize=18,loc='left')\n",
        "\n",
        "# Removing the spines on top, left and right\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd8c005",
      "metadata": {
        "id": "9dd8c005"
      },
      "source": [
        "<i>Above is a plot we created ourselves!</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be8cea0",
      "metadata": {
        "id": "9be8cea0"
      },
      "source": [
        "## 7.3 Visualizing Participant's Runs!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce720159",
      "metadata": {
        "id": "ce720159"
      },
      "source": [
        "Strava's website and app allows the user to visualize the routes for all their activities recorded synced on the Strava application. It is a very interesting feature as it lets the participants view the exact route they took during their activity on an actual map.\n",
        "\n",
        "<img src=\"https://blog.strava.com/wp-content/uploads/2020/03/03-Save-Sync-Share.png\"> <br>\n",
        "\n",
        "\n",
        "Let's try to recreate the plots using the participant's data! <br> <br>\n",
        "Here's a reference to how our overall result will look like\n",
        "\n",
        "<img src=\"https://cf.veloviewer.com/img/veloviewer-strava-segments-homepage.png\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03750fa0",
      "metadata": {
        "id": "03750fa0"
      },
      "source": [
        "Interestingly, Strava provides us with this route data in the form of polylines. In the df_strava dataframe we can see a column called summary_polyline for all the individual activities. We will be using this column for our plot! <br>\n",
        "\n",
        "Polyline is a encoded format that store coordinate location and it has to be decoded before it can be used. If we have a closer look into it, it is a bunch of characters that make no sense to an average reader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d90c263",
      "metadata": {
        "id": "5d90c263"
      },
      "outputs": [],
      "source": [
        "import polyline\n",
        "import folium\n",
        "\n",
        "#Dictionary to save the coordinates of the first ride\n",
        "my_ride = map_summary_polyline_df.iloc[1].get(['map.summary_polyline']).apply(polyline.decode)['map.summary_polyline']\n",
        "\n",
        "#Select one activity to find the centroid of the map.\n",
        "centroid = [\n",
        "    np.mean([coord[0] for coord in my_ride]),\n",
        "    np.mean([coord[1] for coord in my_ride])\n",
        "  ]\n",
        "\n",
        "#Creating a map\n",
        "m = folium.Map(location=centroid, zoom_start=13)\n",
        "\n",
        "# Plot all rides on map\n",
        "for i in range(len(map_summary_polyline_df.head())):\n",
        "    if len(map_summary_polyline_df.iloc[i].get(['map.summary_polyline'])['map.summary_polyline']) == 0:\n",
        "        continue\n",
        "\n",
        "    # Polyline.decode is a function that helps us decode this polyline data to coordinates\n",
        "    my_ride = map_summary_polyline_df.iloc[i].get(['map.summary_polyline']).apply(polyline.decode)['map.summary_polyline']\n",
        "    # We plot the route on the map.\n",
        "    folium.PolyLine(my_ride, color='red').add_to(m)\n",
        "display(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72662856",
      "metadata": {
        "id": "72662856"
      },
      "source": [
        "# 8. Statistical Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "537ccc99",
      "metadata": {
        "id": "537ccc99"
      },
      "source": [
        "Now that we have found, the weekly summary for the participant's activity time, let's plot some graphs to see if there is a correlation between the various metrics of the participants data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922d9a21",
      "metadata": {
        "id": "922d9a21"
      },
      "source": [
        "In order to focus on the just the runs, we will have to clean the dataframe and only keep activities of the type 'Run'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbcd807",
      "metadata": {
        "id": "fcbcd807"
      },
      "outputs": [],
      "source": [
        "strava_df = max_speed_df.merge(max_heartrate_df.get(['start_date','max_heartrate']),on='start_date')\n",
        "\n",
        "def filterRuns(data):\n",
        "    dataframe = []\n",
        "    for i in range(len(data)):\n",
        "        name = data.iloc[i].get('name')\n",
        "        if 'run' in name.lower():\n",
        "            dataframe.append(True)\n",
        "        else:\n",
        "            dataframe.append(False)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "strava_df = strava_df[filterRuns(strava_df)]\n",
        "strava_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d155c0c4",
      "metadata": {
        "id": "d155c0c4"
      },
      "outputs": [],
      "source": [
        "# We will drop the null values and only get the columns we need\n",
        "df_runs_cleaned = strava_df.get(['max_speed','max_heartrate']).dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed50a304",
      "metadata": {
        "id": "ed50a304"
      },
      "source": [
        "As the strava api returns the max_speed values in meters/second and our above plots use miles, we will convert max speed to km/hour to maintain consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7928f0",
      "metadata": {
        "id": "4b7928f0"
      },
      "outputs": [],
      "source": [
        "df_runs_cleaned = df_runs_cleaned.assign(max_speed =\n",
        "                    df_runs_cleaned.get('max_speed').apply(lambda x: x*3.6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfbfab58",
      "metadata": {
        "id": "dfbfab58"
      },
      "source": [
        "Let's plot the cleaned Data Frame below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85938c27",
      "metadata": {
        "id": "85938c27"
      },
      "outputs": [],
      "source": [
        "df_runs_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2820d5",
      "metadata": {
        "id": "4f2820d5"
      },
      "source": [
        "Maybe the heart rate is correlated with how fast you run. Let's test if this hypothesis is true. We will do so by plotting a scatterplot between those two metrics and finding the correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be82bd2",
      "metadata": {
        "id": "8be82bd2"
      },
      "source": [
        "First, we will plot a chart to see if there is a visual correlation between a partcipant's max heart rate and their max speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0abe31",
      "metadata": {
        "id": "0f0abe31"
      },
      "outputs": [],
      "source": [
        "# Setting Figure Size in Seaborn\n",
        "sns.set(rc={'figure.figsize':(16,8)})\n",
        "\n",
        "# Setting Seaborn plot style\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "#Plotting our data\n",
        "plot = sns.regplot(data=df_runs_cleaned, x=\"max_speed\", y=\"max_heartrate\")\n",
        "\n",
        "#Renaming x and y labels\n",
        "plot.set_ylabel(\"Maximum Heart Rate (bpm)\", fontsize = 16)\n",
        "plot.set_xlabel(\"Maximum Speed (km/hr)\", fontsize = 16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43467c5f",
      "metadata": {
        "id": "43467c5f"
      },
      "source": [
        "As we can see from the scatterplot above, our regression line hints that there might be a correlation between maximum speed and maximum heart rate. Let's compute $R^2$ just to see exactly how correlated.\n",
        "\n",
        "We'll follow [this documentation](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.linregress.html) and perform a linear regression to obtain the coefficient of determination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b76987",
      "metadata": {
        "id": "29b76987"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
        "    df_runs_cleaned.get('max_speed'), df_runs_cleaned.get('max_heartrate'))\n",
        "\n",
        "print(f'Slope: {slope:.3g}')\n",
        "print(f'Coefficient of determination: {r_value**2:.3g}')\n",
        "print(f'p-value: {p_value:.3g}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23cb6fee",
      "metadata": {
        "id": "23cb6fee"
      },
      "source": [
        "As we can see that the p-value is slightly over 17% which means that there is not enough evidence to convincingly conclude that that there is a correlation between heart rate and speed. Let's try to locate the outliers in this data and find points that might be skewing our dataset and preventing us from gathering enough evidence to prove our correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbd9a666",
      "metadata": {
        "id": "dbd9a666"
      },
      "source": [
        "# 9. Outlier Detection and Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "511819b5",
      "metadata": {
        "id": "511819b5"
      },
      "source": [
        "Before finding the individual outlier values, it would be interesting to see the summary of our max_speed and max_heartrate parameters. It will give us a clear idea of what values are typical and which values can be considered atypical based on the data that we recieved from Strava."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5df96f",
      "metadata": {
        "id": "5b5df96f"
      },
      "outputs": [],
      "source": [
        "df_runs_cleaned_summary = df_runs_cleaned.describe().get(\n",
        "    ['max_speed','max_heartrate'])\n",
        "df_runs_cleaned_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc81bcf2",
      "metadata": {
        "id": "dc81bcf2"
      },
      "source": [
        "To locate the outliers we will be using a supervised as well as unsupervised algorithm called the Elliptic Envelope. In statistical studies, Elliptic Envelope created an imaginary elliptical area around a given dataset where values inside that imaginary area is considered to be normal data, and anything else is assumed to be outliers. It assumes that the given Data follows a gaussian distribution.\n",
        "\n",
        "\"The main idea is to define the shape of the data and anomalies are those observations that lie far outside the shape. First a robust estimate of covariance of data is fitted into an ellipse around the central mode. Then, the Mahalanobis distance that is obtained from this estimate is used to define the threshold for determining outliers or anomalies.\" [(S. Shriram and E. Sivasankar ,2019, pp. 221-225)](https://ieeexplore.ieee.org/document/9004325)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78697539",
      "metadata": {
        "id": "78697539"
      },
      "outputs": [],
      "source": [
        "from sklearn.covariance import EllipticEnvelope\n",
        "\n",
        "#create the model, set the contamination as 0.02\n",
        "EE_model = EllipticEnvelope(contamination = 0.02)\n",
        "\n",
        "#implement the model on the data\n",
        "outliers = EE_model.fit_predict(df_runs_cleaned[[\"max_speed\", \"max_heartrate\"]])\n",
        "\n",
        "#extract the labels\n",
        "df_runs_cleaned[\"outlier\"] = outliers\n",
        "\n",
        "#change the labels\n",
        "# We use -1 to mark an outlier and +1 for an inliner\n",
        "df_runs_cleaned[\"outlier\"] = df_runs_cleaned[\"outlier\"].apply(\n",
        "    lambda x: str(-1) if x == -1 else str(1))\n",
        "\n",
        "#extract the score\n",
        "df_runs_cleaned[\"EE_scores\"] = EE_model.score_samples(\n",
        "    df_runs_cleaned[[\"max_speed\", \"max_heartrate\"]])\n",
        "\n",
        "#print the value counts for inlier and outliers\n",
        "print(df_runs_cleaned[\"outlier\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ce0fd8",
      "metadata": {
        "id": "19ce0fd8"
      },
      "source": [
        "Below we will replot the df_runs_cleaned dataframe to see how the two new columns were applied to it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f292c728",
      "metadata": {
        "id": "f292c728"
      },
      "outputs": [],
      "source": [
        "df_runs_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01de6e15",
      "metadata": {
        "id": "01de6e15"
      },
      "source": [
        "Now that we have labeled the outliers as -1, let's try to see which values of max heartrate and max speed are being considered as outliers by our Elliptic Envelope Algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62439842",
      "metadata": {
        "id": "62439842"
      },
      "outputs": [],
      "source": [
        "outlier_df = df_runs_cleaned[df_runs_cleaned.get('outlier')=='-1'].get(\n",
        "    ['max_heartrate','max_speed'])\n",
        "outlier_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29ca36a",
      "metadata": {
        "id": "f29ca36a"
      },
      "source": [
        "Using the dataframe above, we can highlight these outlier values in our original scatterplot in order to visually asses which pair/s of max speed and max heart rate values are not following the general trend seen in our scatterplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef579e0c",
      "metadata": {
        "id": "ef579e0c"
      },
      "outputs": [],
      "source": [
        "df_runs_cleaned.drop(outlier_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cd4290",
      "metadata": {
        "scrolled": true,
        "id": "d2cd4290"
      },
      "outputs": [],
      "source": [
        "# Setting Figure Size in Seaborn\n",
        "sns.set(rc={'figure.figsize':(16,8)})\n",
        "\n",
        "# Setting Seaborn plot style\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "# Plotting our data\n",
        "# We will calculate the regression line while not accounting for our outliers\n",
        "plot = sns.regplot(data=df_runs_cleaned.drop(outlier_df.index), x=\"max_speed\",\n",
        "                   y=\"max_heartrate\")\n",
        "\n",
        "#Renaming x and y labels\n",
        "plot.set_ylabel(\"Maximum Heart Rate (bpm)\", fontsize = 16)\n",
        "plot.set_xlabel(\"Maximum Speed (km/hr)\", fontsize = 16)\n",
        "\n",
        "# Plotting the outlier and highlighting it\n",
        "plt.scatter(outlier_df.get('max_speed'),outlier_df.get('max_heartrate'))\n",
        "plt.scatter(outlier_df.get('max_speed'),outlier_df.get('max_heartrate'),\n",
        "            facecolors='red',alpha=.35, s=500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6eb9511",
      "metadata": {
        "id": "d6eb9511"
      },
      "source": [
        "Thus, the points highlighted in red are ones that seem to not be following the general trend of our dataset. Lastly, let's see what the new p-value is after outlier removal!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d7a7e7",
      "metadata": {
        "id": "44d7a7e7"
      },
      "outputs": [],
      "source": [
        "slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
        "    df_runs_cleaned.drop(outlier_df.index).get('max_speed'),\n",
        "    df_runs_cleaned.drop(outlier_df.index).get('max_heartrate'))\n",
        "\n",
        "print(f'Slope: {slope:.3g}')\n",
        "print(f'Coefficient of determination: {r_value**2:.3g}')\n",
        "print(f'p-value: {p_value:.3g}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}